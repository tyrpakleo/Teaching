\documentclass{article}
\input{preamble}

\title{Lookdown construction}
\author{Leo Tyrpak}
\date{September 2024}

\begin{document}

\maketitle
The lookdown construction is a versatile way of endowing a population process with extra information used to keep track of genealogies.
\section{DeFinetti theorem}
When considering sequences of random variables $(X_1,X_2,...)$ all taking values in a space $(E,d)$ which is complete and metrisable it is useful to consider their joint distribution which takes values in $\Omega=E^{\N}$ with $\sigma$-algebra $\cF=\overset{\N}{\times}E$ and $X_n(\omega)=\omega_n$.
\begin{dfn}
    A sequence $X_1,X_2,...$ is called exchangeable if for each $n$ and for each permutation $\pi$ of $\{1,...,n\}$, $(X_1,...,X_n)$ and $(X_{\pi(1)},...,X_{\pi(n)})$.
    A probability measure $\mu\in\cP(E^{\N})$ is called exchangeable if $p_N\#\mu$ is exchangeable for all $N$ where $p_N:E^{\N}\rightarrow E^N$ is the projection of the first $N$ coordinates.
    A probability measure $\mu\in\cP(E^N)$ is exchangeable if $\pi\#\mu=\mu$ for all permutations $\pi$ of $N$ coordinates.
\end{dfn}

\begin{dfn}
    Define $\cE_n$ as the $\sigma$-algebra generated by events that are invariant under permutations leaving $n+1,n+2,...$ fixed.
    More precisely,
    \begin{equation}
        \cE_n=\sigma(A:p_n^{-1}(A)=A,\text{ for all }p_n\in\Pi_n)
    \end{equation}
    where $\Pi_n$ is the set of permutations on $E^{\N}$ that leave $n+1,n+2,...$ fixed.
    
    Define $\cE=\cap_n\cE_n$ to be the exchangeable field.
\end{dfn}
\begin{dfn}
    Define the tail $\sigma$-algebra $\cF_n=\sigma(X_{n+1},X_{n+2},...)$ and $\cF_\infty=\cap_n\cF_n$.
\end{dfn}
\begin{thm}[Kolmogorov $0-1$ law]\label{thm:kolmogorov01}
    If $X_1,X_2,...$ are independent then $\cF_\infty$ is trivial (if $A\in\cF_\infty$ then $\p{A}\in\{0,1\}$).
\end{thm}
\begin{proof}
    \textbf{Step 1:} $\cF_n$ is independent to $\cG_n=\sigma(X_1,...,X_n)$ therefore since $\cF_\infty\subseteq\cF_n$, $\cF_\infty$ is independent to $\cG_n$

    \textbf{Step 2:} By step 1 $\cF_\infty$ is independent of $\cG_\infty=\vee_n\cG_n$ (since $\cup\cG_n$ is a $\pi$ system generating $\cG_\infty$).
    Then $\cF_\infty\subseteq\cG_\infty$ and we are done.
\end{proof}
Let us give some examples of random variables and events which are $\cF_\infty$ measurable; $\limsup{S_n/a_n},\liminf{S_n/a_n}$ where $a_n\rightarrow\infty$ and $A=\{\limsup S_n=\pm \infty\}$.

Here is another $0-1$ law.
\begin{thm}[Hewitt-Savage $0-1$ law]\label{thm:hewitt_savage}
    Suppose $X_1,X_2,...$ are i.i.d. then $\cE$ is trivial.
\end{thm}

For this we will need a little lemma.
For an exchangeable sequence $X_1,X_2,...$ we define
\begin{equation}
    A_n(\phi)=\frac{1}{(n)_k}\sum\phi(X_{i_1},...,X_{i_k}),
\end{equation}
where sum is over distinct integers $1\leq i_1,...,i_k\leq n$ and $(n)_k$ is the number of such sequences.
Note that $(n)_k=n(n-1)...(n-k+1)$ with leading order term $n^k$.

One can check that,
\begin{equation}
    A_n(\phi)=\E{\phi(X_1,...,X_k)|\cE_n}
\end{equation}
\begin{lem}\label{lem:hewitt_helper}
    Suppose $X_1,X_2,...$ are i.i.d. and $\phi$ is bounded. Then, 
    \begin{equation*}
        A_n(\phi)\rightarrow\E{\phi(X_1,...,X_k)} \text{ a.s.}
    \end{equation*}
\end{lem}
\begin{proof}
    \textbf{Step 1:} 
    By the backwards martingale convergence theorem we have,
    \begin{equation}
        A_n(\phi)=\E{\phi(X_1,...,X_k)|\cE_n}\rightarrow\E{\phi(X_1,...,X_k)|\cE} \text{ a.s.},
    \end{equation}
    as $n\rightarrow\infty$.
    
    \textbf{Step 2:} 
    We will show that 
    \begin{equation}\label{eq:intail_sigma}
        \E{\phi(X_1,...,X_k)|\cE}\in\cF_k=\sigma(X_{k+1},X_{k+2},...).
    \end{equation}

    Consider the set $B$ of sequences of indices $i_1,...,i_k$ that have any of the indices $1,...,k$. 
    There are at most $k^2(n-1)_{k-1}$ of them.
    Then,
    \begin{equation}
        \left|\frac{1}{(n)_k}\sum_{(i_1,...,i_k)\in B}\phi(X_{i_1},...,X_{i_k})\right|\leq \frac{k^2(n-1)_{k-1}}{(n)_k}\lVert\phi\rVert_\infty\rightarrow0
    \end{equation}
    Therefore we have that
    \begin{equation}
        \frac{1}{(n)_k}\sum_{(i_1,...,i_k)\notin B}\phi(X_{i_1},...,X_{i_k})\rightarrow \E{\phi(X_1,...,X_k)|\cE}
    \end{equation}
    The LHS is $\cF_k$ by construction and then so is the limit proving the claim \ref{eq:intail_sigma}.

    \textbf{Step 3:} If we show that $\E{\phi(X_1,...,X_k)|\cE}$ is a.s. constant then we are done. 
    We note that $\sigma(X_1,...,X_k)$ is independent of $\sigma(\E{\phi(X_1,...,X_k)|\cE})\subseteq\sigma(X_{k+1},X_{k+2},...)$ and so by the exercise following this we are done.
    
\end{proof}
\textbf{Exercise:} if $\E{X^2}<\infty$ and $\E{X|\cG}\perp X$ then $\E{X|\cG}$ is a.s. constant. (This essentially follows by the interpretation in $L^2$ of conditional probability as the projection onto the subspace $L^2(\Omega,\cG,\bP)$)

\begin{proof}[Pf of \ref{thm:hewitt_savage}]
    We note that by a similar argument to \ref{thm:kolmogorov01} it is enough to show for all $k$ we have $\cE\perp\sigma(X_1,...,X_k)$.

    By \ref{lem:hewitt_helper} we have that for all $\phi$ bounded,
    \begin{equation*}
        \E{\phi(X_1,...,X_k)|\cE}=\E{\phi(X_1,...,X_k)}
    \end{equation*}
    By standard arguments of independence this implies $\cE\perp\sigma(X_1,...,X_k)$.
\end{proof}

\begin{thm}[de Finetti's Theorem]\label{thm:deFinetti}
    If $X_1,X_2,...$ are exchangeable then conditional on $\cE$, $X_1,X_2,...$ are i.i.d.
\end{thm}
\begin{proof}
    \textbf{Step 1:}
    To prove independence we show that
    \begin{equation*}
        \E{\prod_{j=1}^kf_j(X_j)|\cE}=\prod_{j=1}^k\E{f_j(X_j)|\cE}
    \end{equation*}
    Once we have proven independence, it is clear that each of the $X_i$'s has the same distribution given $\cE$.
    This follows by noting the equality,
    \begin{equation*}
        \E{f_1(X_i)1_A}=\E{f_1(X_j)1_A}
    \end{equation*}
    for $A\in\cE$ and $f_1$ bounded which follows by exchangeability and because $A\in\cE_{\max(i,j)}$.

    \textbf{Step 2:}
    Note just as the proof of \ref{lem:hewitt_helper}, we have,
    \begin{equation*}
        A_n(\phi)\rightarrow\E{\phi(X_1,...,X_k)|\cE_n}\text{ a.s.}
    \end{equation*}
    with the caveat that now the Hewitt-Savage law doesn't apply so $\cE$ might be non-trivial.
    Let $f\in C_b(E^{k-1}),g\in C_b(E)$ and set $\phi(x_1,...,x_k)=f(x_1,...,x_{k-1})g(x_k)$.
    Similar convergences hold for $A_n(f)$ and $A_n(g)$

    Let $I_{n,k}$ be the set of all indices $1\leq i_1,...,i_k\leq n$ with $i_j$'s different.
    One can check the equalities;
    \begin{align*}
        (n)_{k-1}A_n(f)nA_n(g)&=\sum_{i\in I_{n,k-1},m}f(X_{i_1},...,X_{i_{k-1}})g(X_m)\\
        &=\sum_{i\in I_{n,k}}f(X_{i_1},...,X_{i_{k-1}})g(X_{i_k})+\sum_{i\in I_{n,k-1}}\sum_{m\in\{i_1,...,i_k\}}f(X_{i_1},...,X_{i_{k-1}})g(X_m)\\
        &=(n)_kA_n(\phi)+(n)_{k-1}\sum_{i=1}^{k-1}A_n(\phi_j),
    \end{align*}
    where $\phi_j(x_1,...,x_{k-1})=f(x_1,...,x_{k-1})g(x_i)$.

    \textbf{Step 3:}
    Divide through by $n^k$ in the equation above and let $n\rightarrow\infty$ to get that,
    \begin{align*}
        \E{f(X_1,...,X_{k-1})|\cE}\E{g(X_k)|\cE}=\E{f(X_1,...,X_{k-1})g(X_k)|\cE}
    \end{align*}
    Now an induction finishes the proof of independence.
\end{proof}
\section{Fleming-Viot example}
Let us discuss an example before moving on to the theory. Let us consider the Fleming-Viot process on $\R$ with mutation. 
\subsection{Description of prelimit model}
We have $N$ particles living on $\R$, with locations $(Y_1(t),...,Y_N(t)$ at time $t=0$. We fix a $\gamma\in[0,1]$.

The dynamics evolve as follows: at rate $1$ for each pair of particles $(i,j)$, individual with index $j$ dies 
and individual with index $i$ has an offspring.
With probability $1-\gamma$ the offspring of $i$ takes on the same location as $i$.
With probability $\gamma$ the offspring of $i$ mutates following a kernel $\eta_N(y_i,dz)$ (which we usually consider to be the kernel of $y_i$
plus some noise which is normally distributed).

We can explicitly write down a generator for this process which takes values in $\R^N$. 
Define for $i\neq j$,
    $$Q_{i,j}^Nf(x_1,...,x_N)=(1-\gamma)f(\theta_{i,j}(y_1,...,y_N))+\gamma\int f(y_1,...,y_{j-1},z,y_{j+1},...,y_N)\eta_N(y_i,dz)-f(y),$$
where $\theta_{i,j}(y)\in\R^N$ is obtained by replacing $y_j$ by $y_i$.

Then the process has generator,
\begin{equation*}
    C_Nf=\frac{1}{2}\sum_{i\neq j}Q_{i,j}f.
\end{equation*}
\begin{dfn}
    The Moran model with $N$ particles started from $\nu_0$ is the unique process with cadlag paths taking values in $\R^N$generator $C_N$ and initial distribution $\nu_0$. 
\end{dfn}
Let us denote this process by $V^N$.

\subsection{Scaling limit}
We will be interested in deriving the Fleming-Viot process as the limit of the sequence of Moran models as $N\rightarrow\infty$.
An essential hypothesis can be stated in terms of operators $B^N$,
\begin{equation*}
    B^Ng(x)=\frac{N\gamma}{2}\int g(z)-g(x)\eta_N(x,dz).
\end{equation*}
We will assume that $B^N$ converges to the generator $B$ of an $\R$-valued Markov process.

\begin{hyp}
    \begin{enumerate}
        \item For the generators $(C_N)_{N\geq1}$ to converge we require,
        $$.$$
        \item 
    \end{enumerate}
\end{hyp}


We ultimately don't look at the specific indices of the individuals and only consider the empirical measure of the process $Z^N(t)=\frac{1}{N}\sum_{k=1}^N\delta_{Y_k(t)}$.

Let $N\rightarrow\infty$ and define $Z$ as
\begin{equation*}
    Z(t)=\lim_{N\rightarrow\infty}Z^N(t),
\end{equation*}
where the limit is taken in distribution.
Then $Z(t)$ is the Fleming-Viot process.
\subsection{Lookdown construction}
Suppose we wanted to create a particle system $(X_1(t),...,X_N(t),X_{N+1}(t),...)\in\R^{\N}$ with the property that the projection to the first $N$ coordinates $(X_1(t),...,X_N(t))\in\R^N$ is a Fleming-Viot process with $N$ individuals. 

Consider the $\R^{\N}$-valued process with generator for $f\in\cD(B)\cap B(\R^m)$
\end{document}