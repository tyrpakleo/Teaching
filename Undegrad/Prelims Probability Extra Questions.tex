\documentclass{article}
\input{preamble}

\title{Prelims Probability extra questions}
\author{Leo Tyrpak}
\date{September 2024}

\begin{document}

\maketitle
\section{Sheets extra questions}

\subsection{Sheet 1 extra}
Do the following from Grimmett:
\begin{itemize}
    \item Q 1.2 (all)
    \item Q 1.3.1, 1.3.6, 1.3.7
\end{itemize}
\subsection{Sheet 2 extra}
Do the following from Grimmett:
\begin{itemize}
    \item Q 1.5.2-1.5.5, 1.5.7, 1.5.9
\end{itemize}

Question 1: Find $4$ events which are $3$-way independent but not $4$-way independent, see if your construction generalises.

Question 5 extra: estimate the probabilities of $(a),(b)$ replacing $13$ by $n$, $26$ by $2n$ and $52$ by $4n$.
These are both related to the Central Limit Theorem and local Central Limit Theorem.
Search them up if you want to find out more.

Question 6 extra:
\begin{itemize}
    \item Prove that if $\{A_i:i\in I\}$ independent then $\{A_i^c:i\in I\}$ independent
    \item Prove that if $\{A_i:i\in I\}$ independent then $\p{\bigcap_{i\geq1}A_i}=\prod_{i\geq1}\p{A_i}$
    \item Using equality $\sum_{n\geq 1}\frac{1}{n^s}=\prod_{p\text{ prime}}\left(1-\frac{1}{p^s}\right)^{-1}$ prove there are infinitely many primes
\end{itemize}

\subsection{Sheet 3 extra}
\section{Sums of random variables}
Suppose $X_1,...,X_n$ are independent and identically distributed (i.i.d.) with $\E{X_1}=0$.
Let $S_n=X_1+...+X_n$.

Suppose $\E{|X_1|^k}<\infty$ for $k\in\N$.
What is $\E{S_n^k}$? What is $M_{S_n}(t)$ in terms of $M_{X_1}(t)$?
\section{Combinatorics}
\begin{enumerate}
    \item For sheet 8 question 7, what happens if $p=a$ or $p=b$? What does the limit approach, why?
    \item Let $U_1,...,U_n$ i.i.d Uniform [0,1] random variables. What is $\p{U_1+...+U_n\leq1}$?
    \item During a game of poker you are dealt a hand of 5 cards uniformly at random. With the convention that aces can be either high or low for a straight. What is the probability of having 1 pair, 2pairs, 3 of a kind, straight, flush, full house, 4 of a kind, straight flush. See \href{https://en.wikipedia.org/wiki/List_of_poker_hands}{Poker hands} for a list of poker hands and what they mean.
    \item 8 pawns are placed uniformly at random on a chess board, no more than one per square. What is the probability that no 2 are in the same row or column?
    \item Players A, B, C throw a uniform dice at random in the order ABCABCABC... Each dice throw is independent. What is the probability that A throws a 6 first? What is the probability that A throws a 6 first and B throws a 6 the second and C is the third to throw a 6?
    \item Each member of a group of n players roll a dice. For any pair of players who roll the same number, the group scores 1 point. Find the mean and variance of the total score of the group. Find the mean and variance of the total score if any pair of players who throw the same number scores that number.
\end{enumerate}
\section{Probabilistic method}
This is also sometimes called probabilistic combinatorics.

Let G=(V,E) be a finite simple graph (so no loops or multiple edges between 2 vertices.
Write $d_v$ for the degree of an edge $v$.
Prove the following:
\begin{enumerate}
    \item An independent set is a set of vertices no 2 of which share an edge. Denote by $\alpha(G)$ the largest independent set. Prove that:
    \begin{equation}
        \alpha(G)\geq\sum_v\frac{1}{d_v+1}
    \end{equation}
    \item For any set $W\subseteq V$, edge $e\in E$:
    \begin{equation}
        1_W(e)=1\text{ if e connects }W \text{ and }W^c
    \end{equation}
    If $N_W=\sum_e1_W(e)$ then prove that there exists $W$ such that $N_W\geq1/2|E|$
\end{enumerate}
We construct a random graph G' by including each edge independently with probability $p\in(0,1)$.
\section{Analysis}
\begin{enumerate}
    \item Prove Bonferroni's inequality
    \begin{equation}
        \p{\bigcup_{r=1}^nA_r}\geq\sum_{r=1}^n\p{A_r}-\sum_{r<k}\p{A_r\cap A_k}
    \end{equation}
    \item Prove Kounia's inequality
    \begin{equation}
        \p{\bigcup_{r=1}^nA_r}\leq\min_k\left\{\sum_{r=1}^n\p{A_r}-\sum_{r\neq k}\p{A_r\cap A_k}\right\}
    \end{equation}
\end{enumerate}

\subsection{Moment generating functions}
Sheet 3 Question 5 asks you to prove that,
\begin{equation*}
    \E{\exp(t X)}=\exp(\lambda(e^t-1))
\end{equation*}
for $X$ a Poisson random variable of parameter $\lambda$.

The expression on the left is called a moment generating function(mgf) and is a powerful analytical tool useful in probability to get concentration inequalities and uniqueness of distributions.

We write $M_X(t)=\E{\exp(tX)}$ for the mgf at $t$.

Calculate the mgf of the following:
\begin{enumerate}
    \item Geo(p)
    \item Ber(p)
    \item Uniform on $\{1,...,n\}$
    \item Bin(n,p)
    \item Exponential rate $\lambda$
    \item Normal random variable mean $\mu$, variance $\sigma^2$
\end{enumerate}
Let $a,b$ be some constants, then what is the mgf of $Z_1=aX+b$ (i.e. $M_{Z_1}(t)$?

Now suppose $X,Y$ are independent, then what is the mgf of $Z_2=X+Y$ (i.e. $M_{Z_2}(t)$)?

You might have noticed that the mgf doesn't always exist, as sometimes the expectation of $\exp(tX)$ is infinite. 
Can you construct an example of a random variable where $\E{\exp(tX)}$ is infinite for all $t\neq0$?

By differentiating (and not worrying about the important analysis that needs to be done in the background but that you will only do later), can you relate the moments of a distribution with the mgf?

Note: a moment is an expression of the form $\E{X^k}$.
First moment is mean $\E{X}$ and second moment is $\E{X^2}$.

Use this to find expressions for the mean and variance of a random variable $X$ using the mgf.
Then check this with the distributions considered above.

Prove that for all $t$:
\begin{equation*}
    \p{X>x}\leq M_X(t)\exp(-tx)
\end{equation*}
Consider $X$ is Ber(p),Geo(p),Bin(n,p),Poi($\lambda$),Exp($\lambda$) and for each of those optimise the right hand side in $t$ to get optimal tail bounds.
\subsection{Probability generating functions}
Sheet 5 Question 4 asks you to calculate the probability generating function of a Geometric distribution.

Calculate the following pgf:
\begin{enumerate}
    \item Ber(p)
    \item Bin(n,p)
    \item Poisson($\lambda$)
    \item Uniform $\{1,...,n\}$
    \item Negative Binomial (r,p) with pdf $p_k={k+r-1 \choose k}(1-p)^kp^r$
\end{enumerate}
Let $a,b$ be some constants, then what is the pgf of $Z_1=aX+b$ (i.e. $M_{Z_1}(t)$?

Now suppose $X,Y$ are independent, then what is the pgf of $Z_2=X+Y$ (i.e. $M_{Z_2}(t)$)?

Can we recover the distribution of a random variable from the pgf?
\section{Miscellaneous}
\subsection{Sheet 3 Question 6}
Sheet 3 Question 6 asks us to find out how long we have to wait on average to see 2 heads in a row, then Sheet 5 Question 5 asks us to calculate the exact distribution.
\begin{enumerate}
    \item How long do you have to wait on average to get k heads in a row where $k\in\N$?
    \item What is $r_n=\p{X>n}$ where $X$ is the first time you see k heads in a row?
    \item Let Z be the first time you see $HTH$. What is $\E{Z}$ and $\p{Z>n}$?
    \item Let Z be the first time you see $HTTH$. What is $\E{Z}$?
    \item If the challenge doesn't prove enough, try to extend your methods to any sequence.
\end{enumerate}


\end{document}